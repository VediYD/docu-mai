{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb30e2ea-0d10-4041-b1b2-47223852b6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vediy\\anaconda3\\envs\\docu-mai\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: The specified module could not be found. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\vediy\\anaconda3\\envs\\docu-mai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# example code snippets\n",
    "X = [\n",
    "    \"def max(a, b): if a > b: return a else return b\",\n",
    "    \"def factorial(n): if n == 0: return 1 else: return n * factorial(n-1)\",\n",
    "    \"def fibonacci(n): if n <= 1: return n else: return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"def is_palindrome(s): return s == s[::-1]\",\n",
    "    \"def bubble_sort(arr): for i in range(len(arr)): for j in range(0, len(arr)-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j]\",\n",
    "    \"def is_prime(n): if n <= 1: return False for i in range(2, int(n**0.5)+1): if n % i == 0: return False return True\",\n",
    "    \"def reverse_list(lst): return lst[::-1]\",\n",
    "    \"def gcd(a, b): while b: a, b = b, a % b return a\",\n",
    "    \"def binary_search(arr, x): l, r = 0, len(arr)-1 while l <= r: mid = (l + r) // 2 if arr[mid] == x: return mid elif arr[mid] < x: l = mid + 1 else: r = mid - 1 return -1\",\n",
    "    \"def sum_list(lst): return sum(lst)\",\n",
    "    \"def celsius_to_fahrenheit(c): return (c * 9/5) + 32\"\n",
    "]\n",
    "y_true = [\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[max] --> B{a > b}\n",
    "        B -->|Yes| C[Return a]\n",
    "        B -->|No| D[Return b]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[factorial n] --> B{n == 0}\n",
    "        B --> |Yes| C[return 1]\n",
    "        B --> |No| D[return n * m]\n",
    "        D --> |m = factorial n-1| A\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[fibonacci n] --> B{n <= 1}\n",
    "        B --> |Yes| C[return n]\n",
    "        B --> |No| D[return i + j]\n",
    "        D --> |i = fibonacci n-1| A\n",
    "        D --> |j = fibonacci n-2| A\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[is_palindrome s] --> B{\"s == s[::-1]\"}\n",
    "        B --> |Yes| C[return True]\n",
    "        B --> |No| D[return False]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[bubble_sort arr] --> B[\"loop i from 0 to len(arr)-1\"]\n",
    "        B --> C[\"loop j from 0 to len(arr)-i-2\"]\n",
    "        C --> |End| B\n",
    "        C --> D{\"arr[j] > arr[j+1]\"}\n",
    "        D --> |Yes| E[\"swap arr[j] and arr[j+1]\"]\n",
    "        D --> |No| F[continue]\n",
    "        E --> F\n",
    "        F --> C\n",
    "        B --> |End| I[return arr]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[is_prime n] --> B{n <= 1}\n",
    "        B --> |Yes| C[return False]\n",
    "        B --> |No| D[\"loop i from 2 to int(n**0.5)+1\"]\n",
    "        D --> E{\"n  % i == 0\"}\n",
    "        E --> |Yes| C\n",
    "        E --> |No| G[continue]\n",
    "        G --> D\n",
    "        D --> |End| F[return True]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[reverse_list lst] --> B[\"return lst[::-1]\"]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[gcd a b] --> B[while b]\n",
    "        B --> C[a, b = b, a % b]\n",
    "        C --> B\n",
    "        B --> |End| D[return a]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[biary_search arr x] --> B[\"l, r = 0, len(arr)-1\"]\n",
    "        B --> C[while l <= r]\n",
    "        C --> |End| D[return -1]\n",
    "        C --> E[\"mid = (l + r) // 2\"]\n",
    "        E --> F{\"arr[mid] == x\"}\n",
    "        F --> |Yes| G[return mid]\n",
    "        F --> |No| H{\"arr[mid] < x\"}\n",
    "        H --> |Yes| I[l = mid + 1]\n",
    "        H --> |No| J[r = mid - 1]\n",
    "        J & I --> K[continue]\n",
    "        K --> C\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[sum_list lst] --> B[\"return sum(lst)\"]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[celsius_to_fahrenheit c] --> B[\"return (c * 9/5) + 32\"]\n",
    "    ```\"\"\"\n",
    "]\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenize the input and target\n",
    "def tokenize_function(source, target):\n",
    "    source_encodings = tokenizer(source, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    target_encodings = tokenizer(target, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    return source_encodings, target_encodings\n",
    "\n",
    "source_encodings, target_encodings = tokenize_function(X, y_true)\n",
    "\n",
    "# Device setup\n",
    "cuda_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", is_decoder=True).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-2,\n",
    "    per_device_train_batch_size=11,\n",
    "    num_train_epochs=200,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "# Prepare the dataset in a format compatible with Trainer\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, source_encodings, target_encodings):\n",
    "        self.source_encodings = source_encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.source_encodings.input_ids[idx].to(device),\n",
    "            'attention_mask': self.source_encodings.attention_mask[idx].to(device),\n",
    "            'labels': self.target_encodings.input_ids[idx].to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ea7ae-f9e0-4950-aab1-8946228b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = CodeDataset(source_encodings, target_encodings)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# New code snippet to translate\n",
    "new_code = \"def max(a, b): if a > b: return a else return b\"\n",
    "\n",
    "# Tokenize the new input\n",
    "new_input = tokenizer(new_code, return_tensors=\"pt\").to(device)\n",
    "\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24858b75-5069-4b0e-a046-190a20a6890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the translated code\n",
    "generated_tokens = model.generate(\n",
    "    input_ids=new_input['input_ids'],\n",
    "    attention_mask=new_input['attention_mask'],\n",
    "    max_length=256,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "generated_code = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04743d-7bb9-4b92-9c53-646e711a7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code snippet to translate\n",
    "new_code = \"def factorial(n): if n == 0: return 1 else: return n * factorial(n-1)\"\n",
    "\n",
    "# Tokenize the new input\n",
    "new_input = tokenizer(new_code, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_tokens = model.generate(\n",
    "    input_ids=new_input['input_ids'],\n",
    "    attention_mask=new_input['attention_mask'],\n",
    "    max_length=256,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "generated_code = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "generated_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e54c6-136d-41cd-a43f-062f942cb063",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "880372b3-da75-474b-8b67-c6daa7bb8167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute and'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = RobertaForCausalLM.from_pretrained(\"FacebookAI/roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=256)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57555c56-da8b-454c-9723-95fd923f58a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     8,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'past_key_values': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prepare_inputs_for_generation(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc2c7453-e478-45a8-a1d7-afc5974a81a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     8,     2,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(inputs.input_ids, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6123d285-4fea-414f-92a1-8b48a94359e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vediy\\anaconda3\\envs\\docu-mai\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vediy\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my dog is cute and I love him. I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure if he's a good dog, but I'm sure he's a good dog.\n",
      "\n",
      "I'm not sure if he's a good dog, but I'm sure he's a good dog. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode the input text\n",
    "inputs = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(**inputs, max_length=256, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487b811-7655-488e-b01d-bc4e75b01ed8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c2d152-853f-49f3-a545-f3d7fccb6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vediy\\anaconda3\\envs\\docu-mai\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: The specified module could not be found. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\vediy\\anaconda3\\envs\\docu-mai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "X = [\n",
    "    \"def max(a, b): if a > b: return a else return b\",\n",
    "    \"def factorial(n): if n == 0: return 1 else: return n * factorial(n-1)\",\n",
    "    \"def fibonacci(n): if n <= 1: return n else: return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"def is_palindrome(s): return s == s[::-1]\",\n",
    "    \"def bubble_sort(arr): for i in range(len(arr)): for j in range(0, len(arr)-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j]\",\n",
    "    \"def is_prime(n): if n <= 1: return False for i in range(2, int(n**0.5)+1): if n % i == 0: return False return True\",\n",
    "    \"def reverse_list(lst): return lst[::-1]\",\n",
    "    \"def gcd(a, b): while b: a, b = b, a % b return a\",\n",
    "    \"def binary_search(arr, x): l, r = 0, len(arr)-1 while l <= r: mid = (l + r) // 2 if arr[mid] == x: return mid elif arr[mid] < x: l = mid + 1 else: r = mid - 1 return -1\",\n",
    "    \"def sum_list(lst): return sum(lst)\",\n",
    "    \"def celsius_to_fahrenheit(c): return (c * 9/5) + 32\"\n",
    "]\n",
    "y_true = [\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[max] --> B{a > b}\n",
    "        B -->|Yes| C[Return a]\n",
    "        B -->|No| D[Return b]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[factorial n] --> B{n == 0}\n",
    "        B --> |Yes| C[return 1]\n",
    "        B --> |No| D[return n * m]\n",
    "        D --> |m = factorial n-1| A\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[fibonacci n] --> B{n <= 1}\n",
    "        B --> |Yes| C[return n]\n",
    "        B --> |No| D[return i + j]\n",
    "        D --> |i = fibonacci n-1| A\n",
    "        D --> |j = fibonacci n-2| A\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[is_palindrome s] --> B{\"s == s[::-1]\"}\n",
    "        B --> |Yes| C[return True]\n",
    "        B --> |No| D[return False]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[bubble_sort arr] --> B[\"loop i from 0 to len(arr)-1\"]\n",
    "        B --> C[\"loop j from 0 to len(arr)-i-2\"]\n",
    "        C --> |End| B\n",
    "        C --> D{\"arr[j] > arr[j+1]\"}\n",
    "        D --> |Yes| E[\"swap arr[j] and arr[j+1]\"]\n",
    "        D --> |No| F[continue]\n",
    "        E --> F\n",
    "        F --> C\n",
    "        B --> |End| I[return arr]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[is_prime n] --> B{n <= 1}\n",
    "        B --> |Yes| C[return False]\n",
    "        B --> |No| D[\"loop i from 2 to int(n**0.5)+1\"]\n",
    "        D --> E{\"n  % i == 0\"}\n",
    "        E --> |Yes| C\n",
    "        E --> |No| G[continue]\n",
    "        G --> D\n",
    "        D --> |End| F[return True]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[reverse_list lst] --> B[\"return lst[::-1]\"]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[gcd a b] --> B[while b]\n",
    "        B --> C[a, b = b, a % b]\n",
    "        C --> B\n",
    "        B --> |End| D[return a]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[biary_search arr x] --> B[\"l, r = 0, len(arr)-1\"]\n",
    "        B --> C[while l <= r]\n",
    "        C --> |End| D[return -1]\n",
    "        C --> E[\"mid = (l + r) // 2\"]\n",
    "        E --> F{\"arr[mid] == x\"}\n",
    "        F --> |Yes| G[return mid]\n",
    "        F --> |No| H{\"arr[mid] < x\"}\n",
    "        H --> |Yes| I[l = mid + 1]\n",
    "        H --> |No| J[r = mid - 1]\n",
    "        J & I --> K[continue]\n",
    "        K --> C\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[sum_list lst] --> B[\"return sum(lst)\"]\n",
    "    ```\"\"\",\n",
    "    \"\"\"```mermaid\n",
    "    graph TD\n",
    "        A[celsius_to_fahrenheit c] --> B[\"return (c * 9/5) + 32\"]\n",
    "    ```\"\"\"\n",
    "]\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea66eee-d4ac-4c53-8411-1779680a5d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:17, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.510400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.376300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Mermaid syntax:\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> <unk> mermaid,<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> <extra_id_0> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> <extra_id_0> <pad><pad><pad><pad><pad><pad><pad><pad><pad> <extra_id_0> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# Load a pre-trained T5 model\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Tokenize the input and target\n",
    "def tokenize_function(source, target):\n",
    "    prompt = [f\"translate python to mermaid: {s}\" for s in source]\n",
    "    source_encodings = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "    target_encodings = tokenizer(target, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "    return source_encodings, target_encodings\n",
    "\n",
    "source_encodings, target_encodings = tokenize_function(X, y_true)\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, source_encodings, target_encodings):\n",
    "        self.source_encodings = source_encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.source_encodings.input_ids[idx].to(device),\n",
    "            'attention_mask': self.source_encodings.attention_mask[idx].to(device),\n",
    "            'labels': self.target_encodings.input_ids[idx].to(device)\n",
    "        }\n",
    "\n",
    "dataset = CodeDataset(source_encodings, target_encodings)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=11,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Example Python code input\n",
    "code_input = \"def max(a, b): if a > b: return a else return b\"\n",
    "\n",
    "# Construct the prompt\n",
    "prompt = f\"translate python to mermaid: {code_input}\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Generate the output using T5\n",
    "outputs = model.generate(inputs.input_ids, max_length=256, num_beams=5, early_stopping=False)\n",
    "\n",
    "# Decode the output to text\n",
    "mermaid_output = tokenizer.decode(outputs[0])\n",
    "\n",
    "# Print the generated Mermaid syntax\n",
    "print(\"Generated Mermaid syntax:\")\n",
    "print(mermaid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556890db-d97f-4b7e-b79c-6ade36ada41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
